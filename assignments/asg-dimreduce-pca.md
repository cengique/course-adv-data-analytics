# Using dimensionality reduction with PCA for analyzing your dataset (due midnight Wed Nov 13)

In the class, we talked about methods like PCA which can find a reduced subset of your dataset that can contain approximately the same information. It can be thought as finding the *essential* features in your data. Think about **what part** of your data can benefit from reducing to its essentials. 

For the questions below, select 2 or more **numeric columns** from your dataset and place them into a new dataframe. If you can't find many numeric columns, you can consider converting a categorical column by quantifying it into values (for example by converting \[bad, neutral, good\] to \[-1, 0, +1\]).

Make a new section in your notebook and do the following questions. Post your notebook link here. You don't need to start a new notebook, so you can continue on the previous one.

# Questions:

1. Find the **covariance matrix** of your numeric dataframe by first centering it and then using a **for loop** over all of its rows as explained in [my slide](https://cengique.github.io/course-adv-data-analytics/module-dimreduce.html#/5/3)  (and on page 8 of the textbook slides or page 189 of [Chapter 7](https://dataminingbook.info/book_html/chap7/book.html)). Each row vector should be transposed and matrix multiplied with its original form (row vector), which should result in a square matrix as wide as the number of your original columns (as in an outer product). Sum all of these together and divide by number of rows minus one. Compare your calculated covariance matrix to one you can obtain with [numpy.cov](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) (transpose your original numeric data before passing) and discuss whether you got it right in your notebook. 
2. Calculate all the **principal components** of your dataset by finding the eigenvalues and eigenvectors of your covariance matrix by using [numpy.linalg.eig](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html). The eigenvector with the largest eigenvalue would be your first principal component. Compare your calculated component to the one you can get from the [PCA module in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Add text blocks or comments to explain your code and create bar plots of the resulting components and their `explained_variance` amounts. Was PCA able to reduce your dimensions? Discuss briefly in your notebook. 3. Use the `transform()` method in the `sklearn` PCA module to transform your numeric dataset onto the PC bases and create two scatter plots comparing the original vs transformed datasets. You can use 2D or 3D plots. Explain if you see an improvement of representing your data.

Since CREATE symposium is early this year, you don't have much time to include this in your poster. But if you can, do it. In your notebook and on your poster  focus on the **main question** you want to highlight in your analysis this semester. Give a brief explanation of what you expect to find, and perform data munging and visualization steps to create some results plots that are properly annotated. These don't need to be your final results as we will have one more week to improve them. Add text blocks properly describing your steps and the resulting graphs.
