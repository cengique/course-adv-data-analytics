<!doctype html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <title>Module 8 - Big Data algorithms and tools: Spark - ITEC 4220 Advanced Data Analytics</title>

  <link rel="stylesheet" href="css/reveal.css">
  <link rel="stylesheet" href="css/theme/black.css">

  <!-- Theme used for syntax highlighting of code -->
  <link rel="stylesheet" href="lib/css/zenburn.css">

  <!-- Printing and PDF exports -->
  <script>
  var link = document.createElement( 'link' );
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
  document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <style>
  .reveal strong {
    color: lightblue;
  }
  .home-link {
    position: absolute;
    left: 20px;
    bottom: 20px; 
    font-size: 24pt;
  }
  .reveal section.side-by-side figure {
    position: absolute;
  }
  .reveal section.side-by-side figure:first-of-type {
    left: 5%;
  }
  .reveal section.side-by-side figure:nth-of-type(2) {
    right: 5%;
  }
</style>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section data-markdown>
          <textarea data-template>
            <img src="images/Spark_cover.png" style="float: right; max-width: auto; height: 500px; margin: 1em;"/>
              ## ITEC 4220 - Advanced Data Analytics

              ### Module 8 - Big Data algorithms and tools: Spark

              #### Cengiz Gunay, Rick Price, Fall 2020
              #### Reading: Ch 2 Spark - The definitive guide
          </textarea>
        </section>
        <section> <!-- Begin vertical for Hadoop Streaming -->
          <section data-markdown>
            <textarea data-template>
              ### Python example for Hadoop streaming (Mapper)
              - Need to run `chmod a+x map.py` to make it executable
              ```python
              #! /usr/bin/python
              # Map function
              import sys
              for line in sys.stdin:
                  val = line.strip()
                  (date, temp) = (val[10:14], val[15:20])
                  print ("%s\t%s" % (date, temp))
              ```
              - Streaming version will take standard input and print out results
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### Python example for Hadoop streaming (Reducer)
              - Need to run `chmod a+x reduce.py` to make it executable

              ```python
              #! /usr/bin/python
              # Reduce function
              import sys
              (last_key, max_val) = (None, -99999)
              for line in sys.stdin:
                (key, val) = line.strip().split("\t")
                if last_key and last_key != key:
                  print ("%s\t%s" % (last_key, max_val))
                  (last_key, max_val) = (key, int(float(val)))
                else:
                  (last_key, max_val) = (key, max(max_val, int(float(val))))
                  if last_key:
                    print ("%s\t%s" % (last_key, max_val))
              ```
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### How to run it

              On the command line without Hadoop:
              ```bash
              % cat Daily.txt | python map.py | sort | python reduce.py
              0727	0
              0728	92
              0729	90
              0730	91
              0731	106
              0801	108
              ```
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### With Hadoop

              ```bash
              % bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
                -input input/Daily.txt \
                -output output \
                -mapper map.py \
                -reducer reduce.py
              ```
              ```bash
              0727    0
              0728    92
              0729    90
              0730    91
              0731    106
              0801    108
              ```
            </textarea>
          </section>
        </section>
        <section data-markdown>
          <textarea data-template>
            ### Hadoop is dead. Long live Hadoop!

            - Hadoop created an ecosystem of projects:
              - Avro: Data serialization system
              - Flume: Work with data streams
              - Sqoop: Interface with traditional relational DBs
              - Hive: SQL queries converted to MapReduce
              - [Pig](https://www.slideshare.net/kevinweil/hadoop-pig-and-twitter-nosql-east-2009/): Hadoop processing with custom high-level language
              - Parquet: Columnar storage for nested data
              - Crunch: High-level API for using Hadoop
              - [Kafka](https://kafka.apache.org/): Distributed streaming platform
              - [Spark](https://spark.apache.org/): Another distributed computing framework
            - We will give a brief intro to Spark
            <!-- .element: class="fragment" data-fragment-index="2" -->
          </textarea>
        </section>
    <section> <!-- Begin vertical for Spark Explanation-->
          <section data-markdown>
            <textarea data-template>
              <img src="images/Spark_cover.png" style="float: right; max-width: auto; height: 500px;"/>
              ### Spark

              - Since 2009
              - Written in Scala running on JVM
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### Features

              - Speed
                - Up to 100 times faster than Hadoop
                - Supports in-memory storage of intermediate results
              - Multiple languages supported
                - Python, Scala, R, Java
              - Advanced Analytics
                 - SQL queries
                 - Streaming data
                 - Machine learning
                 - Graph algorithms
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Spark and Hadoop Differences

              Hadoop:
              - Batch process
              - Processes data from the HDFS and writes intermediate results to HDFS
              - Results are written to the HDFS

              Spark:
              - Can read data from streaming, local file system, HDFS or other distributed file system
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Spark Components
              <img src="images/SparkComponents.png" style="float: right; max-width: auto; height: 500px;"/>
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Resilient Distributed Datasets (RDD)
              - Fundamental data structure for Spark
                - Immutable distributed collection of objects
              - Divided into logical partitions
                - Can be computed on different nodes
                - Can contain any data type or Python, Java, or Scala
                - Can contain User Defined Types (UDTs)
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Spark SQL
              - Integrates relational processing
              - Supports queries in SQL and HQL (Hive Query Language)
              - Supports various data sources
                - CSV, Text, JSON, etc.
              - Libraries
                - DataSource API
                - DataFrame API
                - Interpreter and Optimizer
                - SQL Service
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Spark Streaming
              - Process real-time streaming data
              - Fundamental stream unit is DStream
                - Series of RDDs to process real-time data
              - Viewed as a continually growing table
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### MLib (Machine Learning Library)

              - Tools
                - ML Algorithms
                  - Classification, Regression, Clustering, Collaborative Filtering
                - Featurization
                  - Feature extraction, transformations, and dimension reduction
              - Pipelines
                - Contruct and training ML pipelines
              - Persistence
                - Saving and loading algorithms, models and pipelines
              - Utilities
                - Linear algebra, statistics, data handling
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### GraphX
              - API for graphs and graph-parallel computation
              - Extends RDD with a Resilient Distributed Property Graph
              - Can have parallel edges to define multiple relationships between vertices
              ### GraphFrames
              - Similar to GraphX but built on top of DataFrames
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### DataSet and DataFrame
              - DataSet
                - Distributed collection of data
                - Strong typing
                - Available in Scala and Java
              - DataFrame
                - DataSet organized into named columns
                - Representation of DataSet in rows
                - Available in Scala, Java, Python and R
            </textarea>
          </section>
        </section> <!-- end Spark Explanation -->
        <section> <!-- Spark coding -->
          <section data-markdown>
            <textarea data-template>
              ### Practice time!

              Log into our ITECLAB host, and run:
              
              ```docker exec -it gunayspark bin/pyspark```

              which will give:

              ```bash
              Python 3.6.15 (default, Sep 24 2021, 11:37:20) 
              [GCC 8.3.0] on linux
              Type "help", "copyright", "credits" or "license" for more information.
              Welcome to
                 ____             __    
                / __/__  ___ _____/ /__
               _\ \/ _ \/ _ `/ __/  '_/
              /__ / .__/\_,_/_/ /_/\_\   version 3.1.2
                 /_/

              Using Python version 3.6.15 (default, Sep 24 2021 11:37:20)
              Spark context Web UI available at http://640b8cafa9e9:4041
              Spark context available as 'sc' (master = local[*], app id = local-1634165976411).
              SparkSession available as 'spark'.
              >>> 
              ```
            </textarea>
          </section>
          <section data-markdown>
          <textarea data-template>
            ### Spark works with _resilient distributed datasets_ (RDDs)

            - A line count program example:
            ```python
            >>> lines = sc.textFile("README.md") # Create an RDD called lines
            >>> lines.count() # Count the number of items in this RDD
            127 # May vary based on your version of Spark
            >>> lines.first() # First item in this RDD, i.e. first line of README.md
            u'# Apache Spark'
            ```
            - Parallel operation completely transparent!
            - <!-- .element: class="fragment" data-fragment-index="2" --> `sc` is the `SparkContext` _driver_ to access Spark
            and create RDDs
            -  <!-- .element: class="fragment" data-fragment-index="3" --> RDD is broken into pieces to run `count()` in parallel
          </textarea>
         </section>
         <section data-markdown>
          <textarea data-template>
            ### Inside Spark

            <img src="images/Spark_Ch2_fig2-3_components.png" style="max-width: auto; height: 300px;"/>
            - Try another example with a custom filter function: <!-- .element: class="fragment" data-fragment-index="2" -->

            ```python
            >>> lines = sc.textFile("README.md")
            >>> sparkLines = lines.filter(lambda line: "Spark" in line) # like grep on Hadoop
            >>> sparkLines.count() # a lower number than before
            >>> sparkLines.first() 
            u'# Apache Spark'
            ```
            <!-- .element: class="fragment" data-fragment-index="2" -->
        </textarea>
         </section>

                <section data-markdown>
                  <textarea data-template>
                    ### Map-Reduce operations generalize to <br/> Transformation-Action with RDDs

                    _Transformations_ create new RDDs; e.g.:

                    ```python
                    >>> sparkLines = lines.filter(lambda line: "Spark" in line)
                    ```

                    _Actions_ calculate results from RDDs; e.g.:

                    ```python
                    >>> sparkLines.first()
                    ```
                  </textarea>
                </section>
                <section data-markdown>
                  <textarea data-template>
                    ### RDD transformations: `map()` and `filter()`

                    <img src="images/Spark_Ch3_fig3-2_map-filter.png" style="max-width: auto; height: 300px;"/>

                    ```python
                    >>> inputRDD = sc.parallelize([1, 2, 3, 4]) # Create a numeric input RDD

                    ```

                    ```python
                    >>> outputRDD = inputRDD.map(lambda x: x * x) # MAP
                    ```
                    <!-- .element: class="fragment" data-fragment-index="2" -->

                    ```python
                    >>> outputRDD = inputRDD.filter(lambda x: x != 1) # FILTER
                    ```
                    <!-- .element: class="fragment" data-fragment-index="3" -->

                    ```python
                    >>> outputRDD.collect() # Show all results
                    ```
                    <!-- .element: class="fragment" data-fragment-index="4" -->

                  </textarea>
                </section>
                <section data-markdown>
                  <textarea data-template>
                    ### RDD transformations (Continued)

                    - For `map()`, if multiple outputs for each input, then use `flatMap()`
                    - <!-- .element: class="fragment" data-fragment-index="2" -->
                    Set operations:
                        - `distinct()`
                        - `union()`
                        - `intersection()`
                        - `subtract()`
                        - `cartesian()` -> expensive
                  </textarea>
                </section>
                <section data-markdown>
                  <textarea data-template>
                    ### RDD actions

                    - `reduce()`: Takes two inputs and outputs one; output is a scalar
                    ```python
                    >>> sum = inputRDD.reduce(lambda x, y: x + y)
                    >>> sum
                    10
                    ```
                    - <!-- .element: class="fragment" data-fragment-index="2" -->
                    `fold()` also takes a _zero_ value for initialization
                    ```python
                    sum = inputRDD.fold(0, lambda x, y: x + y)
                    ```
                    <!-- .element: class="fragment" data-fragment-index="2" -->
                    - <!-- .element: class="fragment" data-fragment-index="3" -->
                    `aggregate()` asks for accummulation and combine functions. Example that calculates a running sum and count of elements to calculate an average value:

                    ```python
                    sumCount = inputRDD.aggregate((0, 0),
                      (lambda acc, value: (acc[0] + value, acc[1] + 1)),
                      (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])))
                    return sumCount[0] / float(sumCount[1])
                    ```
                    <!-- .element: class="fragment" data-fragment-index="3" -->
                  </textarea>
                </section>

        </section> <!-- end spark coding -->
        <section> <!-- SQL and DataFrame -->
                <section data-markdown>
                  <textarea data-template>
                    ### RDDs can act as DataFrames!

                    - Can read/write data in JSON, CSV, Hive Tables, and Parquet
                    - _DataFrame_ holds semi-structured data, supports SQL-like querying

                    ```python
                    df = spark.read.json("examples/src/main/resources/people.json") # also see read.csv()
                    # Displays the content of the DataFrame to stdout
                    df.show()
                    # +----+-------+
                    # | age|   name|
                    # +----+-------+
                    # |null|Michael|
                    # |  30|   Andy|
                    # |  19| Justin|
                    # +----+-------+
                    #
                    # Select only the "name" column
                    df.select("name").show()
                    # Select people older than 21
                    df.filter(df['age'] > 21).show()
                    ```

                  </textarea>
                </section>
                <section data-markdown>
                  <textarea data-template>
                    ### Spark can run SQL queries in parallel

                    - SQL interface to be used inside or outside Spark <br/> (e.g. JDBC connection or from Tableau)
                    ```python
                    # Register the DataFrame as a SQL temporary view
                    df.createOrReplaceTempView("people")
                    sqlDF = spark.sql("SELECT * FROM people")
                    sqlDF.show()
                    # +----+-------+
                    # | age|   name|
                    # +----+-------+
                    # |null|Michael|
                    # |  30|   Andy|
                    # |  19| Justin|
                    # +----+-------+
                    ```
                  </textarea>
                </section>
          <section data-markdown>
            <textarea data-template>
              ### Spark Dataframes can be convert from and to Pandas Dataframes

              - [Pandas](https://pandas.pydata.org/) provides the standard Dataframe object in Python
              - Enable Apache Arrow in Spark [to use Pandas objects](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html)

              ```python
import numpy as np
import pandas as pd

# Enable Arrow-based columnar data transfers
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")

# Generate a Pandas DataFrame
pdf = pd.DataFrame(np.random.rand(100, 3))

# Create a Spark DataFrame from a Pandas DataFrame using Arrow
df = spark.createDataFrame(pdf)

# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow
result_pdf = df.select("*").toPandas()
              ```
            </textarea>
          </section>
        </section> <!-- end SQL and DataFrame -->
     <section data-markdown>
      <textarea data-template>
       ### Closing notes for Spark
       <img src="images/Spark_cover.png" style="float: right; max-width: auto; height: 500px;"/>

       Spark also provides:
        - [Real-time processing via streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html) (Chapter 10)
        - [Machine learning library, _MLlib_](https://spark.apache.org/docs/latest/ml-guide.html) (Chapter 11)

        More resources:
         - [Book's Github repo with examples](https://github.com/databricks/learning-spark)
         - [Spark 3.0.0 API documentation](https://spark.apache.org/docs/3.0.0/) - <br>
            JetStream image version
         - [Spark Quick Start](https://spark.apache.org/docs/latest/quick-start.html)
         - [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
      </textarea>
     </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        math: {
  mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js', // https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js
  config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1280, // 960
        height: 720, // 700

        // Factor of the display size that should remain empty around the content
        margin: 0.1,
        history: true,

        // Bounds for smallest/largest possible scale to apply to content
        minScale: 0.2,
        maxScale: 1.5,

        dependencies: [
  { src: 'plugin/markdown/marked.js' },
  { src: 'plugin/markdown/markdown.js' },
  { src: 'plugin/notes/notes.js', async: true },
  { src: 'plugin/highlight/highlight.js',
    async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                // MathJax
  { src: 'plugin/math/math.js', async: true }
        ]
      });
    </script>
          <a class="home-link" href="index.html">Home</a>
  </body>
</html>
